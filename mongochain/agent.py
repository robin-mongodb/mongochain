"""Main MongoAgent class for mongochain."""

import json
from typing import Optional, Callable

from .config import AgentConfig, MemoryConfig
from .embeddings import VoyageEmbeddings
from .llm import LLMClient
from .memory import MemoryStore


class MongoAgent:
    """An AI agent with MongoDB-backed memory.
    
    Each agent gets its own MongoDB database with separate collections for
    different memory types. All memories are user-specific (by email/ID).
    
    Memory Types:
    - conversation_history: Raw chat log with embeddings (90-day TTL)
    - short_term_memory: Session summaries (7-day TTL)
    - long_term_memory: Persistent user facts (no TTL)
    
    Attributes:
        name: Agent name
        persona: Current agent personality/system prompt
        collaborators: List of agent names this agent can access
    """
    
    def __init__(
        self,
        name: str,
        persona: str,
        mongo_uri: str,
        voyage_api_key: str,
        llm_api_key: str,
        llm_provider: str = "openai",
        llm_model: Optional[str] = None,
        collaborators: Optional[list[str]] = None,
        memory_config: Optional[MemoryConfig] = None
    ):
        """Initialize a MongoAgent.
        
        Args:
            name: Agent name (becomes the MongoDB database name)
            persona: The agent's personality/system prompt
            mongo_uri: MongoDB Atlas connection string
            voyage_api_key: Voyage AI API key for embeddings
            llm_api_key: API key for the LLM provider
            llm_provider: LLM provider ("openai", "anthropic", or "google")
            llm_model: Specific model to use (uses provider default if None)
            collaborators: List of agent names this agent can read memories from
            memory_config: Optional memory configuration
        """
        self._config = AgentConfig(
            name=name,
            persona=persona,
            mongo_uri=mongo_uri,
            voyage_api_key=voyage_api_key,
            llm_api_key=llm_api_key,
            llm_provider=llm_provider,
            llm_model=llm_model,
            collaborators=collaborators or []
        )
        
        self.name = name
        self.persona = persona
        self.collaborators = list(self._config.collaborators)
        self._mongo_uri = mongo_uri
        
        # Initialize components
        self._embeddings = VoyageEmbeddings(api_key=voyage_api_key)
        self._llm = LLMClient(
            provider=llm_provider,
            api_key=llm_api_key,
            model=llm_model
        )
        self._memory = MemoryStore(
            mongo_uri=mongo_uri,
            db_name=self._config.db_name,
            config=memory_config
        )
        
        # Track current session for summarization
        self._session_messages: dict[str, list] = {}  # user_id -> messages
        
        # Registered tools: name -> {"func": callable, "description": str, "parameters": dict}
        self._tools: dict[str, dict] = {}
        
        # Setup database and collections
        self._setup()
    
    def _setup(self):
        """Setup the agent's database and collections."""
        status = self._memory.setup_collections()
        
        # Save agent metadata to MongoDB
        self._memory.save_agent_metadata(
            name=self.name,
            persona=self.persona,
            llm_provider=self._llm.provider,
            llm_model=self._llm.model,
            collaborators=self.collaborators,
            description=None
        )
        
        print(f"Agent: {self.name} created. Check database '{self._config.db_name}' in your MongoDB cluster.")
        
        if status["collections_created"]:
            print(f"  Collections created: {', '.join(status['collections_created'])}")
        for vs in status["vector_index_status"]:
            print(f"  Vector index - {vs}")
    
    def chat(self, user_id: str, message: str) -> str:
        """Send a message and get a response.
        
        Args:
            user_id: User's email or unique identifier
            message: The user's message
            
        Returns:
            The agent's response
        """
        # Initialize session tracking for this user if needed
        if user_id not in self._session_messages:
            self._session_messages[user_id] = []
        
        # Generate embedding for the user message
        message_embedding = self._embeddings.embed(message)
        
        # Store user message in conversation history (with embedding)
        self._memory.store_conversation(
            user_id=user_id,
            role="user",
            content=message,
            embedding=message_embedding
        )
        
        # Track in session
        self._session_messages[user_id].append({"role": "user", "content": message})
        
        # Build context from all memory sources
        context = self._build_context(user_id, message, message_embedding)
        
        # Build messages for LLM
        messages = self._build_messages(user_id, message, context)
        system_prompt = self._build_system_prompt(user_id, context)
        
        # Check if tools are available
        if self._tools:
            response = self._chat_with_tools(messages, system_prompt)
        else:
            response = self._llm.chat(messages, system_prompt=system_prompt)
        
        # Generate embedding for the response
        response_embedding = self._embeddings.embed(response)
        
        # Store assistant response (with embedding)
        self._memory.store_conversation(
            user_id=user_id,
            role="assistant",
            content=response,
            embedding=response_embedding
        )
        
        # Track in session
        self._session_messages[user_id].append({"role": "assistant", "content": response})
        
        # Extract and store user facts in long-term memory (async-like, non-blocking conceptually)
        self._extract_and_store_user_facts(user_id, message, response)
        
        # Update short-term summary periodically (every 5 exchanges)
        if len(self._session_messages[user_id]) >= 10:  # 5 user + 5 assistant
            self._update_short_term_summary(user_id)
        
        return response
    
    def _chat_with_tools(self, messages: list[dict], system_prompt: str) -> str:
        """Handle chat with tool calling support.
        
        Args:
            messages: The messages to send
            system_prompt: The system prompt
            
        Returns:
            The final response text
        """
        # Build tool definitions for the LLM
        tools = [
            {
                "name": name,
                "description": tool["description"],
                "parameters": tool["parameters"]
            }
            for name, tool in self._tools.items()
        ]
        
        # First call to LLM
        result = self._llm.chat_with_tools(messages, tools, system_prompt)
        
        # If it's a tool call, execute it and get the result
        if result["type"] == "tool_call":
            tool_name = result["name"]
            tool_args = result["arguments"]
            
            if tool_name in self._tools:
                try:
                    # Execute the tool
                    tool_result = self._tools[tool_name]["func"](**tool_args)
                    
                    # Add tool result to messages and get final response
                    messages.append({
                        "role": "assistant",
                        "content": f"I'll use the {tool_name} tool to help answer this."
                    })
                    messages.append({
                        "role": "user", 
                        "content": f"Tool result from {tool_name}: {tool_result}"
                    })
                    
                    # Get final response incorporating tool result
                    final_response = self._llm.chat(messages, system_prompt=system_prompt)
                    return final_response
                    
                except Exception as e:
                    # Tool execution failed, inform the user
                    return f"I tried to use the {tool_name} tool but encountered an error: {str(e)}"
            else:
                return f"Tool '{tool_name}' was requested but is not available."
        
        # Regular text response
        return result["content"]
    
    def chat_stream(self, user_id: str, message: str):
        """Send a message and stream the response.
        
        Args:
            user_id: User's email or unique identifier
            message: The user's message
            
        Yields:
            Chunks of the agent's response text
            
        Note:
            After iteration completes, memories are saved automatically.
        """
        from typing import Generator
        
        # Initialize session tracking for this user if needed
        if user_id not in self._session_messages:
            self._session_messages[user_id] = []
        
        # Generate embedding for the user message
        message_embedding = self._embeddings.embed(message)
        
        # Store user message in conversation history (with embedding)
        self._memory.store_conversation(
            user_id=user_id,
            role="user",
            content=message,
            embedding=message_embedding
        )
        
        # Track in session
        self._session_messages[user_id].append({"role": "user", "content": message})
        
        # Build context from all memory sources
        context = self._build_context(user_id, message, message_embedding)
        
        # Build messages for LLM
        messages = self._build_messages(user_id, message, context)
        
        # Stream response from LLM and collect full response
        full_response = []
        for chunk in self._llm.chat_stream(messages, system_prompt=self._build_system_prompt(user_id, context)):
            full_response.append(chunk)
            yield chunk
        
        # After streaming completes, save the response
        response = "".join(full_response)
        
        # Generate embedding for the response
        response_embedding = self._embeddings.embed(response)
        
        # Store assistant response (with embedding)
        self._memory.store_conversation(
            user_id=user_id,
            role="assistant",
            content=response,
            embedding=response_embedding
        )
        
        # Track in session
        self._session_messages[user_id].append({"role": "assistant", "content": response})
        
        # Extract and store user facts in long-term memory
        self._extract_and_store_user_facts(user_id, message, response)
        
        # Update short-term summary periodically
        if len(self._session_messages[user_id]) >= 10:
            self._update_short_term_summary(user_id)
    
    def _build_context(
        self,
        user_id: str,
        query: str,
        query_embedding: list[float]
    ) -> dict:
        """Build context from all memory sources.
        
        Args:
            user_id: User's email/ID
            query: The current query
            query_embedding: Embedding of the query
            
        Returns:
            Dict with context from each memory type
        """
        context = {
            "short_term": [],
            "long_term": [],
            "conversation": [],
            "collaborator_memories": {}
        }
        
        # Get recent short-term summaries (what happened recently)
        short_term = self._memory.search_short_term(user_id, query_embedding, limit=3)
        context["short_term"] = short_term
        
        # Get relevant long-term memories (user facts)
        long_term = self._memory.search_long_term(user_id, query_embedding)
        context["long_term"] = long_term
        
        # Get recent conversation history
        conversation = self._memory.get_conversation_history(user_id, limit=10)
        context["conversation"] = [
            {"role": m["role"], "content": m["content"]}
            for m in conversation
        ]
        
        # Get collaborator memories if any
        for collab_name in self.collaborators:
            try:
                collab_memories = self._get_collaborator_memories(collab_name, user_id, query_embedding)
                if collab_memories:
                    context["collaborator_memories"][collab_name] = collab_memories
            except Exception:
                pass
        
        return context
    
    def _get_collaborator_memories(
        self,
        agent_name: str,
        user_id: str,
        query_embedding: list[float]
    ) -> list[dict]:
        """Get relevant memories from a collaborator agent."""
        collab_config = AgentConfig(
            name=agent_name,
            persona="",
            mongo_uri=self._mongo_uri,
            voyage_api_key="",
            llm_api_key="",
        )
        
        collab_memory = MemoryStore(
            mongo_uri=self._mongo_uri,
            db_name=collab_config.db_name
        )
        
        try:
            memories = collab_memory.search_long_term(user_id, query_embedding, limit=3)
            return memories
        except Exception:
            return collab_memory.get_user_long_term(user_id, limit=3)
        finally:
            collab_memory.close()
    
    def _build_system_prompt(self, user_id: str, context: dict) -> str:
        """Build the system prompt including user context."""
        base_prompt = f"""You are an AI agent named {self.name}.

{self.persona}

You have access to memories about this user and past conversations.
When you learn something significant about the user (preferences, facts about them, their work, skills, etc.), 
naturally incorporate this knowledge into your responses."""

        # Add user facts from long-term memory
        if context["long_term"]:
            facts = "\n".join(f"- {m['content']}" for m in context["long_term"])
            base_prompt += f"\n\nWhat you know about this user:\n{facts}"
        
        # Add recent session context from short-term memory
        if context["short_term"]:
            summaries = "\n".join(
                f"- {m['summary']}" for m in context["short_term"]
            )
            base_prompt += f"\n\nRecent interactions with this user:\n{summaries}"
        
        return base_prompt
    
    def _build_messages(
        self,
        user_id: str,
        current_message: str,
        context: dict
    ) -> list[dict]:
        """Build the message list for the LLM."""
        messages = []
        
        # Add collaborator context if available
        if context["collaborator_memories"]:
            collab_context = []
            for collab_name, memories in context["collaborator_memories"].items():
                if memories:
                    collab_context.append(
                        f"Information from {collab_name}:\n" +
                        "\n".join(f"- {m['content']}" for m in memories)
                    )
            
            if collab_context:
                messages.append({
                    "role": "user",
                    "content": "[Shared knowledge from other agents]\n" + "\n\n".join(collab_context)
                })
                messages.append({
                    "role": "assistant",
                    "content": "I'll incorporate this shared knowledge in my response."
                })
        
        # Add recent conversation history (excluding current message)
        for msg in context["conversation"][:-1] if context["conversation"] else []:
            messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })
        
        # Add current message
        messages.append({
            "role": "user",
            "content": current_message
        })
        
        return messages
    
    def _extract_and_store_user_facts(
        self,
        user_id: str,
        user_message: str,
        assistant_response: str
    ):
        """Use LLM to extract and store significant user facts."""
        extraction_prompt = f"""Analyze this conversation exchange and extract any significant facts about the user.

User message: {user_message}
Assistant response: {assistant_response}

Extract facts about the user such as:
- Their job/role/profession
- Their company or organization
- Technical skills or expertise
- Preferences or interests
- Projects they're working on
- Personal details they shared

Return a JSON array of facts. Each fact should be a complete sentence.
If there are no significant facts, return an empty array: []

Example output: ["User works as a DevOps engineer", "User is interested in MongoDB optimization"]
Only return the JSON array, nothing else."""

        try:
            result = self._llm.chat(
                [{"role": "user", "content": extraction_prompt}],
                system_prompt="You are a fact extraction assistant. Only output valid JSON arrays."
            )
            
            # Parse the JSON response
            facts = json.loads(result.strip())
            
            if isinstance(facts, list) and facts:
                for fact in facts:
                    if isinstance(fact, str) and len(fact) > 10:
                        # Check if similar fact already exists
                        fact_embedding = self._embeddings.embed(fact)
                        existing = self._memory.search_long_term(
                            user_id, fact_embedding, limit=1
                        )
                        
                        # Only store if no similar fact exists (score < 0.9)
                        if not existing or existing[0].get("score", 0) < 0.9:
                            self._memory.store_long_term(
                                user_id=user_id,
                                content=fact,
                                embedding=fact_embedding,
                                category="user_fact",
                                metadata={"source": "auto_extracted"}
                            )
        except (json.JSONDecodeError, Exception):
            # Silently fail - fact extraction is best-effort
            pass
    
    def _update_short_term_summary(self, user_id: str):
        """Generate and store a summary of the current session."""
        if user_id not in self._session_messages or not self._session_messages[user_id]:
            return
        
        messages = self._session_messages[user_id]
        conversation_text = "\n".join(
            f"{m['role'].upper()}: {m['content'][:200]}" for m in messages[-10:]
        )
        
        summary_prompt = f"""Summarize this conversation session concisely.

{conversation_text}

Provide a JSON response with:
- "summary": A 1-2 sentence summary of what was discussed
- "topics": Array of main topics discussed
- "actions": Array of actions taken or outcomes

Example:
{{"summary": "Discussed MongoDB indexing strategies and helped debug a slow query.", "topics": ["MongoDB", "indexing", "performance"], "actions": ["explained compound indexes", "suggested query optimization"]}}

Only return valid JSON."""

        try:
            result = self._llm.chat(
                [{"role": "user", "content": summary_prompt}],
                system_prompt="You are a conversation summarizer. Only output valid JSON."
            )
            
            summary_data = json.loads(result.strip())
            
            summary_text = summary_data.get("summary", "Session summary")
            summary_embedding = self._embeddings.embed(summary_text)
            
            self._memory.store_short_term(
                user_id=user_id,
                summary=summary_text,
                topics_discussed=summary_data.get("topics", []),
                actions_taken=summary_data.get("actions", []),
                embedding=summary_embedding
            )
            
            # Clear session messages after summarizing
            self._session_messages[user_id] = []
            
        except (json.JSONDecodeError, Exception):
            pass
    
    # ==================== Public Methods ====================
    
    def set_persona(self, new_persona: str):
        """Change the agent's persona dynamically.
        
        Updates both the in-memory persona and persists to MongoDB.
        """
        self.persona = new_persona
        self._config.persona = new_persona
        self._memory.update_agent_persona(new_persona)
        print(f"Agent {self.name}'s persona updated.")
    
    def set_description(self, description: str):
        """Set a human-readable description for the agent.
        
        Args:
            description: A brief description of the agent's purpose
        """
        # Update the full metadata with new description
        self._memory.save_agent_metadata(
            name=self.name,
            persona=self.persona,
            llm_provider=self._llm.provider,
            llm_model=self._llm.model,
            collaborators=self.collaborators,
            description=description
        )
        print(f"Agent {self.name}'s description updated.")
    
    def add_collaborator(self, agent_name: str):
        """Add an agent as a collaborator.
        
        Updates both the in-memory list and persists to MongoDB.
        """
        if agent_name not in self.collaborators:
            self.collaborators.append(agent_name)
            self._memory.update_agent_collaborators(self.collaborators)
            print(f"Added {agent_name} as a collaborator for {self.name}.")
    
    def remove_collaborator(self, agent_name: str):
        """Remove an agent from collaborators.
        
        Updates both the in-memory list and persists to MongoDB.
        """
        if agent_name in self.collaborators:
            self.collaborators.remove(agent_name)
            self._memory.update_agent_collaborators(self.collaborators)
            print(f"Removed {agent_name} from collaborators for {self.name}.")
    
    def store_user_memory(
        self,
        user_id: str,
        content: str,
        category: str = "general"
    ):
        """Manually store a fact about a user in long-term memory.
        
        Args:
            user_id: User's email/ID
            content: The fact/information to store
            category: Category (e.g., "preference", "fact", "skill", "note")
        """
        embedding = self._embeddings.embed(content)
        self._memory.store_long_term(
            user_id=user_id,
            content=content,
            embedding=embedding,
            category=category,
            metadata={"source": "manual"}
        )
        print(f"Memory stored for user {user_id} in category '{category}'.")
    
    def get_user_memories(
        self,
        user_id: str,
        category: Optional[str] = None,
        limit: int = 10
    ) -> list[dict]:
        """Get stored memories for a user.
        
        Args:
            user_id: User's email/ID
            category: Optional category filter
            limit: Max memories to return
            
        Returns:
            List of memory documents
        """
        return self._memory.get_user_long_term(user_id, limit, category)
    
    def get_user_stats(self, user_id: str) -> dict:
        """Get memory statistics for a user."""
        return self._memory.get_user_stats(user_id)
    
    def get_stats(self) -> dict:
        """Get overall agent statistics."""
        return self._memory.get_stats()
    
    def get_metadata(self) -> Optional[dict]:
        """Get the agent's persisted metadata from MongoDB.
        
        Returns:
            Dict with agent metadata including:
            - name: Agent name
            - persona: Current personality/system prompt
            - llm_provider: LLM provider name
            - llm_model: LLM model name
            - collaborators: List of collaborator agent names
            - description: Optional description
            - tools: List of registered tool names
            - created_at: When the agent was first created
            - updated_at: When metadata was last modified
        """
        return self._memory.get_agent_metadata()
    
    # ==================== Tool Management ====================
    
    def register_tool(
        self,
        name: str,
        func: Callable,
        description: str,
        parameters: dict
    ):
        """Register a custom tool/function for the agent to use.
        
        The tool will be persisted to MongoDB (metadata only) and available
        for the agent to call during chat.
        
        Args:
            name: Unique tool name (e.g., "get_weather")
            func: The Python function to call
            description: Human-readable description of what the tool does
            parameters: JSON Schema describing the function parameters
            
        Example:
            def get_weather(location: str, unit: str = "celsius") -> str:
                # ... call weather API ...
                return f"Weather in {location}: 22Â°{unit[0].upper()}"
            
            agent.register_tool(
                name="get_weather",
                func=get_weather,
                description="Get the current weather for a location",
                parameters={
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "City name, e.g., 'London'"
                        },
                        "unit": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "Temperature unit"
                        }
                    },
                    "required": ["location"]
                }
            )
        """
        # Store function reference in memory
        self._tools[name] = {
            "func": func,
            "description": description,
            "parameters": parameters
        }
        
        # Persist tool metadata to MongoDB
        self._memory.register_tool(name, description, parameters)
        
        print(f"Tool '{name}' registered for agent {self.name}.")
    
    def get_tools(self) -> list[dict]:
        """Get all registered tools.
        
        Returns:
            List of tool definitions (from MongoDB)
        """
        return self._memory.get_tools()
    
    def remove_tool(self, name: str):
        """Remove a registered tool.
        
        Args:
            name: Tool name to remove
        """
        # Remove from runtime
        if name in self._tools:
            del self._tools[name]
        
        # Remove from MongoDB
        if self._memory.remove_tool(name):
            print(f"Tool '{name}' removed from agent {self.name}.")
        else:
            print(f"Tool '{name}' not found.")
    
    def clear_user_memories(
        self,
        user_id: str,
        memory_type: Optional[str] = None
    ):
        """Clear memories for a specific user.
        
        Args:
            user_id: User's email/ID
            memory_type: "conversation", "short_term", "long_term", or None for all
        """
        if memory_type == "conversation":
            self._memory.clear_user_conversation(user_id)
        elif memory_type == "short_term":
            self._memory.clear_user_short_term(user_id)
        elif memory_type == "long_term":
            self._memory.clear_user_long_term(user_id)
        else:
            self._memory.clear_user_all(user_id)
        print(f"Cleared {memory_type or 'all'} memories for user {user_id}.")
    
    def end_session(self, user_id: str):
        """End a user session and save the summary.
        
        Call this when a user's session ends to ensure the session
        summary is saved to short-term memory.
        """
        if user_id in self._session_messages and self._session_messages[user_id]:
            self._update_short_term_summary(user_id)
            print(f"Session ended for {user_id}. Summary saved to short-term memory.")
    
    def __repr__(self) -> str:
        return f"MongoAgent(name='{self.name}', provider='{self._llm.provider}')"
